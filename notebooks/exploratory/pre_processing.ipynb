{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Import Necessary Libraries**\n",
    "\n",
    "Start by importing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "#import openai\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Define File Paths**\n",
    "\n",
    "Set the input and output file paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../../data/raw/fullrjokesdata.json'\n",
    "output_file = '../../data/processed/joke_selection_untaged.json'\n",
    "joke_embeddings_file='../../data/processed/joke_embeddings.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Step 3: Specify Relevant Columns**\n",
    "\n",
    "List the columns we want to retain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = ['id', 'title', 'selftext', 'ups', 'score', 'created_utc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Step 4: Process and clean the Data**\n",
    "\n",
    "Since the dataset is large, we'll read and process it line by line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing jokes: 1064928it [00:37, 28711.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total jokes after filtering: 88833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the cleaning function\n",
    "def clean_joke_text(title, selftext):\n",
    "    \"\"\"\n",
    "    Clean the full text of a joke to prepare it for embeddings and classification.\n",
    "    Skips the joke if a URL is found, if selftext is missing/empty, or if selftext is '[deleted]'.\n",
    "    \"\"\"\n",
    "    # Check if selftext is missing, empty, or marked as '[deleted]'\n",
    "    if not selftext.strip() or selftext == '[deleted]':  # Skips if selftext is empty or marked as '[deleted]'\n",
    "        return None\n",
    "    # Combine title and selftext\n",
    "    full_text = f\"{title} {selftext}\".strip()\n",
    "    # List of URL indicators\n",
    "    url_indicators = ['http://', 'https://', 'www.']\n",
    "    # Check if the joke contains a URL\n",
    "    if any(indicator in full_text for indicator in url_indicators):\n",
    "        return None  # Return None if a URL is found\n",
    "    # Convert text to lowercase\n",
    "    clean_text = full_text.lower()\n",
    "    # Remove URLs (if any slipped through)\n",
    "    clean_text = re.sub(r'http\\S+|www.\\S+', '', clean_text)\n",
    "    # Remove any unwanted special characters (optional: modify according to your needs)\n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', clean_text)\n",
    "    # Replace multiple spaces, newlines, or tabs with a single space\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "# Initialize a counter for the number of jokes processed\n",
    "jokes_count = 0\n",
    "\n",
    "# Open the input and output files\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "     open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "    # Iterate over each line (each joke)\n",
    "    for line in tqdm(infile, desc='Processing jokes'):\n",
    "        try:\n",
    "            # Parse the JSON line\n",
    "            joke = json.loads(line)\n",
    "\n",
    "            # Filter jokes with more than 50 score\n",
    "            if joke.get('score', 0) > 50:\n",
    "                title = joke.get('title', '')\n",
    "                selftext = joke.get('selftext', '')\n",
    "\n",
    "                # Clean the joke text (this will skip jokes with '[deleted]' selftext, URLs, or empty selftext)\n",
    "                clean_joke = clean_joke_text(title, selftext)\n",
    "                if clean_joke is None:\n",
    "                    continue  # Skip jokes with invalid selftext or URLs\n",
    "\n",
    "                # Create a new joke dictionary with the cleaned text\n",
    "                filtered_joke = {\n",
    "                    'id': joke.get('id'),\n",
    "                    'full_joke': clean_joke,\n",
    "                    'ups': joke.get('ups'),\n",
    "                    'score': joke.get('score'),\n",
    "                    'created_utc': joke.get('created_utc')\n",
    "                }\n",
    "\n",
    "                # Write the filtered joke to the output file\n",
    "                json.dump(filtered_joke, outfile)\n",
    "                outfile.write('\\n')  # Write each joke on a new line\n",
    "                \n",
    "                jokes_count += 1\n",
    "        except json.JSONDecodeError:\n",
    "            # Skip lines that are not valid JSON\n",
    "            continue\n",
    "\n",
    "print(f\"Total jokes after filtering: {jokes_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5: Verify the Output**\n",
    "\n",
    "To make sure the data has been correctly processed, we will read a few lines from the processed file to ensure everything worked correctly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '9p7em', 'full_joke': 'husband asks wife what would you do if i hit the lottery a husband asks his wifewhat would you do if i hit the lotto she replies id take half and leave your assthe husband saysokay i just won 12 bucks on this scratch offheres 6 bux now get the fuck out', 'ups': 62, 'score': 62, 'created_utc': 1254242875.0}\n",
      "{'id': 'a0ut6', 'full_joke': 'dont do that one day a little girl is outside with her father she claps her hands together and said daddy i killed a butterfly her father replied dont do that butterflies are our friends no butter for a week a little while later the girl was playing and she clapped her hands and said daddy daddy i killed a honeybee her father said dont do that honeybees are our friends no honey for a week later on that day the girl and her father were in the kitchen the girls mother joined them the mother stamped her foot and said i killed a cockroach then the little girl said to her father should i tell her or do you want to', 'ups': 62, 'score': 62, 'created_utc': 1257339411.0}\n",
      "{'id': 'a26ky', 'full_joke': 'whats big gray and unimportant an irrelephant', 'ups': 63, 'score': 63, 'created_utc': 1257689962.0}\n",
      "{'id': 'avq0r', 'full_joke': 'at the end of the tax year the irs office sent an inspector to audit the books of a local hospital at the end of the tax year the irs office sent an inspector to audit the books of a local hospital while the irs agent was checking the books he turned to the cfo of the hospital and said i notice you buy a lot of bandages what do you do with the end of the roll when theres too little left to be of any usegood question noted the cfo we save them up and send them back to the bandage company and every now and then they send us a free box of bandagesoh replied the auditor somewhat disappointed that his unusual question had a practical answer but on he went in his obnoxious way what about all these plaster purchases what do you do with whats left over after setting a cast on a patientah yes replied the cfo realizing that the inspector was trying to trap him with an unanswerable question we save it and send it back to the manufacturer and every now and then they send us a free package of plasteri see replied the auditor thinking hard about how he could fluster the knowitall cfo well he went on what do you do with all the leftover foreskins from the circumcisions you performhere too we do not waste answered the cfo what we do is save all the little foreskins and send them to the irs office and about once a year they send us a complete dick', 'ups': 139, 'score': 139, 'created_utc': 1264781519.0}\n",
      "{'id': 'b04s6', 'full_joke': 'a cruise on the pacific goes all wrong a cruise on the pacific goes all wrong the ship sinks and there are only 3 survivors jim tom and susie they manage to swim to a small island and they live there for a couple of years doing whats natural for men and women to do after several years of casual sex all the time susie felt absolutely horrible about what she was doing she felt having sex with both jim and tom was so immoral and bad that she killed herself it was tragic but jim and tom managed to get through it after a while jim and toms resistance to natures urgings waned and the inevitable happened well a couple more years went by and jim and tom began to feel absolutely horrible about what they were doing so they buried susie', 'ups': 111, 'score': 111, 'created_utc': 1265760137.0}\n"
     ]
    }
   ],
   "source": [
    "with open(output_file, 'r', encoding='utf-8') as f:\n",
    "    for _ in range(5):\n",
    "        line = f.readline()\n",
    "        joke = json.loads(line)\n",
    "        print(joke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6: Generate embeddings**\n",
    "\n",
    "Generate embeddings for each joke and save them to a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\University\\Aalto\\Machine_learning\\Project\\joke_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dell\\.cache\\huggingface\\hub\\models--Alibaba-NLP--gte-Qwen2-1.5B-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct:\n",
      "- tokenization_qwen.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "This modeling file requires the following packages that were not found in your environment: flash_attn. Run `pip install flash_attn`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the pre-trained gte-Qwen2-1.5B-instruct model and tokenizer\u001b[39;00m\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlibaba-NLP/gte-Qwen2-1.5B-instruct\u001b[39m\u001b[38;5;124m'\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAlibaba-NLP/gte-Qwen2-1.5B-instruct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Set the maximum sequence length (adjust based on the model)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8192\u001b[39m\n",
      "File \u001b[1;32md:\\University\\Aalto\\Machine_learning\\Project\\joke_venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:551\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[0;32m    550\u001b[0m     class_ref \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mauto_map[\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n\u001b[1;32m--> 551\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m     _ \u001b[38;5;241m=\u001b[39m hub_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(pretrained_model_name_or_path):\n",
      "File \u001b[1;32md:\\University\\Aalto\\Machine_learning\\Project\\joke_venv\\Lib\\site-packages\\transformers\\dynamic_module_utils.py:502\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[1;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[0;32m    500\u001b[0m     code_revision \u001b[38;5;241m=\u001b[39m revision\n\u001b[0;32m    501\u001b[0m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m final_module \u001b[38;5;241m=\u001b[39m \u001b[43mget_cached_module_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_class_in_module(class_name, final_module)\n",
      "File \u001b[1;32md:\\University\\Aalto\\Machine_learning\\Project\\joke_venv\\Lib\\site-packages\\transformers\\dynamic_module_utils.py:327\u001b[0m, in \u001b[0;36mget_cached_module_file\u001b[1;34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# Check we have all the requirements in our environment\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m modules_needed \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_imports\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_module_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Now we move the module inside our cached dynamic modules.\u001b[39;00m\n\u001b[0;32m    330\u001b[0m full_submodule \u001b[38;5;241m=\u001b[39m TRANSFORMERS_DYNAMIC_MODULE_NAME \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m submodule\n",
      "File \u001b[1;32md:\\University\\Aalto\\Machine_learning\\Project\\joke_venv\\Lib\\site-packages\\transformers\\dynamic_module_utils.py:182\u001b[0m, in \u001b[0;36mcheck_imports\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    179\u001b[0m         missing_packages\u001b[38;5;241m.\u001b[39mappend(imp)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_packages) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis modeling file requires the following packages that were not found in your environment: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    184\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_packages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Run `pip install \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_packages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    185\u001b[0m     )\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_relative_imports(filename)\n",
      "\u001b[1;31mImportError\u001b[0m: This modeling file requires the following packages that were not found in your environment: flash_attn. Run `pip install flash_attn`"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained gte-Qwen2-1.5B-instruct model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('Alibaba-NLP/gte-Qwen2-1.5B-instruct', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('Alibaba-NLP/gte-Qwen2-1.5B-instruct', trust_remote_code=True).eval()\n",
    "\n",
    "# Set the maximum sequence length (adjust based on the model)\n",
    "max_length = 8192\n",
    "\n",
    "# Function to embed jokes\n",
    "def embed_joke(joke_text):\n",
    "    # Tokenize the joke text\n",
    "    input_data = tokenizer(joke_text, padding=\"longest\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get model outputs\n",
    "        outputs = model(**input_data)\n",
    "        \n",
    "        # Use last token pooling as described in the documentation\n",
    "        attention_mask = input_data[\"attention_mask\"]\n",
    "        left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "        if left_padding:\n",
    "            joke_embedding = outputs.last_hidden_state[:, -1]\n",
    "        else:\n",
    "            sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "            joke_embedding = outputs.last_hidden_state[torch.arange(outputs.last_hidden_state.shape[0]), sequence_lengths]\n",
    "\n",
    "        # Normalize the embedding\n",
    "        joke_embedding = F.normalize(joke_embedding, p=2, dim=1)\n",
    "\n",
    "    return joke_embedding.squeeze().tolist()\n",
    "\n",
    "# Limit the number of jokes to 10 for testing\n",
    "max_jokes_to_process = 10\n",
    "jokes_processed = 0\n",
    "\n",
    "# Open the cleaned jokes file and prepare the embeddings output file\n",
    "with open('cleaned_jokes.jsonl', 'r', encoding='utf-8') as infile, \\\n",
    "     open('joke_embeddings_sample.jsonl', 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "    for line in tqdm(infile, desc='Processing jokes'):\n",
    "        if jokes_processed >= max_jokes_to_process:\n",
    "            break  # Stop after processing 10 jokes\n",
    "\n",
    "        try:\n",
    "            joke = json.loads(line)\n",
    "            full_joke = joke['full_joke']\n",
    "\n",
    "            # Generate embedding for the joke\n",
    "            embedding = embed_joke(full_joke)\n",
    "\n",
    "            # Add the embedding to the joke dictionary\n",
    "            joke['embedding'] = embedding\n",
    "\n",
    "            # Write the joke with the embedding to the new output file\n",
    "            json.dump(joke, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "            # Increment the counter\n",
    "            jokes_processed += 1\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            # Skip lines that are not valid JSON\n",
    "            continue\n",
    "\n",
    "print(f\"Embedding generation completed! Processed {jokes_processed} jokes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Select a Test Set of High-Scoring Jokes\n",
    "\n",
    "In this step, we will load the processed jokes from the file `joke_selection_untaged.json` and extract a small test set of jokes with high scores. The purpose of this test set is to explore tagging strategies and test our heuristics on jokes that received positive feedback from users. We'll define a threshold for \"high-scoring\" jokes and randomly select a small subset for experimentation.\n",
    "\n",
    "#### Criteria for Selecting the Test Set:\n",
    "- Jokes with a score above a certain threshold (e.g., `score > 90`).\n",
    "- Randomly select 10 jokes to form the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '5iwrnf', 'full_joke': 'i dont believe in hitting my children as punishment so i send them to school in a justin beiber shirt and crocs and let the other kids beat them instead', 'ups': 107, 'score': 107, 'created_utc': 1482010581.0}, {'id': '26c070', 'full_joke': 'a man finds a penguin on the road a man finds a penguin on the road side thinking the penguin is lost the man takes it drives until finding a police officer and asks what to do take it to the zoo replies the officer one week latter the policer officer sees the man driving with the penguin by his side what are you doing with that penguin i told you to bring him to the zoo i did exactly that he loved it and now we are going to the movies', 'ups': 492, 'score': 492, 'created_utc': 1400882930.0}, {'id': '6q7ikn', 'full_joke': 'an old lady dies and goes to heaven shes chatting it up with stpeter at the pearly gates when all of a sudden she hears the most awful bloodcurdling screams dont worry about that says st peter its only someone having the holes put into her shoulder blades for wings the old lady looks a little uncomfortable but carries on with the conversation ten minutes later there are more blood curdling screams oh my god says the old lady now what is happening not to worry says st peter shes just having her head drilled to fit the halo i cant do this says the old lady im going to hell you cant go there says st peter youll be raped and sodomized maybe so says the old lady but ive already got the holes for that', 'ups': 329, 'score': 329, 'created_utc': 1501283671.0}, {'id': '3mc7gm', 'full_joke': 'the pope walks into a mosque the imam asks why the wrong faith', 'ups': 117, 'score': 117, 'created_utc': 1443191073.0}, {'id': '66oqqj', 'full_joke': 'theres a mouse named in and a mouse named out how does out know that in has died instincts', 'ups': 2462, 'score': 2462, 'created_utc': 1492776045.0}, {'id': '6q2xch', 'full_joke': 'the pharmacist a young man had been dating a girl for some time and they finally decide that its time for them to have sex they decide that hes going to meet her parents have dinner with the family and then they will leave and find a nice quiet place to do the deed the young man goes to the pharmacy to buy condoms but because hes a virgin he has no idea what to buy fortunately the pharmacist is a friendly older man who helps him pick out a pack of condoms and wishes the young man good luck that evening everyone sits down at the dinner table and begin to say grace the young man bows and is saying grace for 5 minutes then 10 then 15 all without ever raising his head after 15 minutes his girlfriend leans over and says you never told me you were this religious and you the young man replies never told me your father was a pharmacist', 'ups': 160, 'score': 160, 'created_utc': 1501237369.0}, {'id': '2wtsi5', 'full_joke': 'adam is a little lonely about a month or so after adam was introduced to eden god and adam are meeting for dinner adam expresses his admiration for the plants and the animals and the joy and beauty of it all but admits that there is one little thing that he feels sad about he feels a tiny bit lonely god quickly points out that he is already working on a solution it is called a woman and is stunning to behold beautiful and slim would make company for adam would care for him when hes sick attend to cooking and cleaning make love to him whenever he wanted and basically be a joy to be around adam is suitably impressed and expresses his eagerness for this woman thing to be created he is practically beside himself there is a catch though says god to create the woman i described i need both of your legs and at least one arm adam hems and haws for a while and then asks what can i get for one rib', 'ups': 302, 'score': 302, 'created_utc': 1424658210.0}, {'id': 'ashro8', 'full_joke': 'why cant miss piggy count to 100 because when she reaches 69 she gets a frog in her throat', 'ups': 318, 'score': 318, 'created_utc': 1550623810.0}, {'id': '8ewphe', 'full_joke': 'whats the difference between sex and gender i didnt have gender with your mom', 'ups': 96, 'score': 96, 'created_utc': 1524687358.0}, {'id': '3u539f', 'full_joke': 'wife texts husband on a cold winter morning windows frozen wont open husband texts back gently pour some lukewarm water over it and then gently tap edges with hammer wife texts back 10 minutes later computer really messed up now', 'ups': 12420, 'score': 12420, 'created_utc': 1448405736.0}, {'id': 'a307r4', 'full_joke': 'to the man in the wheelchair who stole my camouflage jacket you can hide but you cant run', 'ups': 701, 'score': 701, 'created_utc': 1543926261.0}, {'id': '296h2d', 'full_joke': 'einstein heisenberg pascal and newton are playing hide and seek einstein covers his eyes and begins counting while heisenberg and pascal run off and hide newton takes out some chalk and marks a square on the ground with a side length of exactly 1 meter then sits down inside the square when einstein is finished counting and sees newton sitting on the ground he yells ha ive found you newton newton however replies no you havent youve found pascal nbs d uou 1 lsd 1', 'ups': 102, 'score': 102, 'created_utc': 1403813468.0}, {'id': '6x3v6m', 'full_joke': 'franks been drinking too much at the dinner party and decides to give a toast to his wife to my wife the love of my life and the sexiest woman i know but its too bad only one of those three is here tonight there was a burst of laughter from the crowd but franks wife took it in stride raising here glass for a toast of her own to my husband a good provider and the father of my children too bad only one of those could make it', 'ups': 166, 'score': 166, 'created_utc': 1504143427.0}, {'id': 'egxiv9', 'full_joke': 'a guy sat next to me on the train and pulled a out a photo of his wife and said shes beautiful isnt she i said if you think she is beautiful you should see my missus mate he said why is she a stunner i said no shes an optician', 'ups': 1539, 'score': 1539, 'created_utc': 1577573144.0}, {'id': '9ntmwg', 'full_joke': 'will glass coffins ever be popular remains to be seen', 'ups': 205, 'score': 205, 'created_utc': 1539436181.0}, {'id': '1k89x4', 'full_joke': 'a rough and tough cowboy hitches his horse outside a saloon spurs ringing up the stairs the door swings open and he sits down on a stool gimme a beer bottle of whisky after he drinks his fair share we walks back out to unhitch his horse a second later the swinging doors bust open and a bullet tears through the roof all right you sons of bitches whos the coward that stole my horse the bar fell silent some ducked under tables no one he shouted im gonna have another beer and a shot and if my horse aint back out there when im dun im gonna have to do what i did back in odessa an i dun dont wanna do what i had to do back in odessa he said coldly some of the locals shifted restlessly and after the cowboy finished his drinks he walked back outside low and behold his horse was out there he started saddling him up hopped on and was getting ready to spur out of town when the bar keep spoke up hhhey mmmister he stuttered wwwhat did you have to do back in odessa the cowboy flicked his cigar i had to walk home', 'ups': 156, 'score': 156, 'created_utc': 1376339096.0}, {'id': '4x8apm', 'full_joke': 'girls night out two wives go out for girls night both got drunk started walking home and had to go to the bathroom they stopped at a cemetery but had nothing to wipe with one used her panties and the other grabbed a wreath off a grave the next morning one husband calls the other and says no more girls night out my wife came back with no panties you think you have it bad says the other mine came back with a card stuck in her crack that read from all of us at the fire station we will never forget you', 'ups': 1735, 'score': 1735, 'created_utc': 1470926228.0}, {'id': 'a5wgvc', 'full_joke': 'why do cows have hooves instead of feet because they lactose', 'ups': 119, 'score': 119, 'created_utc': 1544727665.0}, {'id': '2j1hpx', 'full_joke': 'semen sample an 85yearold man was requested by his doctor for a sperm count as part of his physical exam the doctor gave the man a jar and said take this jar home and bring back a semen sample tomorrow the next day the 85yearold man reappeared at the doctors office and gave him the jar which was as clean and empty as on the previous day the doctor asked what happened and the man explained well doc its like thisfirst i tried with my right hand but nothing then i tried with my left hand but still nothing then i asked my wife for help she tried with her right hand then with her left still nothing she tried with her mouth first with the teeth in then with her teeth out still nothing we even called up arleen the lady next door and she tried too first with both hands then an armpit and she even tried squeezin it between her knees but still nothing the doctor was shocked you asked your neighbor the old man replied yep none of us could get the jar open', 'ups': 1929, 'score': 1929, 'created_utc': 1413135302.0}, {'id': '748778', 'full_joke': 'she said shell go out with me when pigs can fly but she also said men were pigs so i dont know what shes waiting for', 'ups': 130, 'score': 130, 'created_utc': 1507124479.0}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the threshold for high-scoring jokes\n",
    "score_threshold = 90\n",
    "test_set_size = 20\n",
    "\n",
    "# Load the filtered jokes and gather high-scoring jokes\n",
    "high_scoring_jokes = []\n",
    "\n",
    "with open(output_file, 'r', encoding='utf-8') as infile:\n",
    "    for line in infile:\n",
    "        joke = json.loads(line)\n",
    "        if joke.get('score', 0) > score_threshold:\n",
    "            high_scoring_jokes.append(joke)\n",
    "\n",
    "# Randomly select a subset of high-scoring jokes for the test set\n",
    "test_set = random.sample(high_scoring_jokes, min(test_set_size, len(high_scoring_jokes)))\n",
    "\n",
    "# Display the selected test set of jokes\n",
    "print(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7: Tagging Jokes Using ChatGPT**\n",
    "\n",
    "In this step, we'll use OpenAI's GPT model to automatically tag our selected test set of jokes based on predefined categories. We'll utilize the OpenAI API to send each joke to the model and receive the corresponding tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
