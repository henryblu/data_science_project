{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Import Necessary Libraries**\n",
    "\n",
    "Start by importing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import logging\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv('../../vars.env')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Define File Paths**\n",
    "\n",
    "Set the input and output file paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../../data/raw/fullrjokesdata.json'\n",
    "output_file = '../../data/processed/joke_selection_untaged.json'\n",
    "joke_embeddings_file='../../data/processed/joke_embeddings.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Step 3: Specify Relevant Columns**\n",
    "\n",
    "List the columns we want to retain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = ['id', 'title', 'selftext', 'ups', 'score', 'created_utc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Step 4: Process and clean the Data**\n",
    "\n",
    "Since the dataset is large, we'll read and process it line by line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_joke_text(title, selftext):\n",
    "    \"\"\"\n",
    "    Clean the full text of a joke to prepare it for embeddings and classification.\n",
    "    Skips the joke if a URL is found, if selftext is missing/empty, or if selftext is '[deleted]'.\n",
    "    \"\"\"\n",
    "    if not selftext.strip() or selftext == '[deleted]':\n",
    "        return None\n",
    "    \n",
    "    full_text = f\"{title} {selftext}\".strip()\n",
    "    \n",
    "    url_indicators = ['http://', 'https://', 'www.']\n",
    "    if any(indicator in full_text for indicator in url_indicators):\n",
    "        return None\n",
    "    \n",
    "    clean_text = full_text.lower()\n",
    "    clean_text = re.sub(r'http\\S+|www.\\S+', '', clean_text)\n",
    "    \n",
    "    # Replace triple dots with a space\n",
    "    clean_text = clean_text.replace('...', ' ')\n",
    "    \n",
    "    # Remove any remaining special characters\n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', clean_text)\n",
    "    \n",
    "    # Replace multiple spaces, newlines, or tabs with a single space\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "# Initialize a counter for the number of jokes processed\n",
    "jokes_count = 0\n",
    "\n",
    "# Open the input and output files\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "     open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    \n",
    "    # Iterate over each line (each joke)\n",
    "    for line in tqdm(infile, desc='Processing jokes'):\n",
    "        try:\n",
    "            joke = json.loads(line)\n",
    "            \n",
    "            if joke.get('score', 0) > 50:\n",
    "                title = joke.get('title', '')\n",
    "                selftext = joke.get('selftext', '')\n",
    "                \n",
    "                clean_joke = clean_joke_text(title, selftext)\n",
    "                if clean_joke is None:\n",
    "                    continue\n",
    "                \n",
    "                filtered_joke = {\n",
    "                    'id': joke.get('id'),\n",
    "                    'full_joke': clean_joke,\n",
    "                    'ups': joke.get('ups'),\n",
    "                    'score': joke.get('score'),\n",
    "                    'created_utc': joke.get('created_utc')\n",
    "                }\n",
    "                \n",
    "                json.dump(filtered_joke, outfile)\n",
    "                outfile.write('\\n')\n",
    "                \n",
    "                jokes_count += 1\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "print(f\"Total jokes after filtering: {jokes_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5: Verify the Output**\n",
    "\n",
    "To make sure the data has been correctly processed, we will read a few lines from the processed file to ensure everything worked correctly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'r', encoding='utf-8') as f:\n",
    "    for _ in range(5):\n",
    "        line = f.readline()\n",
    "        joke = json.loads(line)\n",
    "        print(joke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generate Embeddings via OpenAI**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def prepare_batch_files(input_file, batch_size=10000, output_folder='./batch_files'):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    batch_number = 0\n",
    "    batch = []\n",
    "    total_jokes = 0\n",
    "\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "            for line in tqdm(infile, desc=\"Preparing batches\"):\n",
    "                try:\n",
    "                    joke = json.loads(line)\n",
    "                    # Create the JSONL structure with `custom_id`, `method`, `url`, and `body`\n",
    "                    payload = {\n",
    "                        \"custom_id\": joke['id'],\n",
    "                        \"method\": \"POST\",\n",
    "                        \"url\": \"/v1/embeddings\",\n",
    "                        \"body\": {\n",
    "                            \"model\": \"text-embedding-3-small\",\n",
    "                            \"input\": joke['full_joke'],\n",
    "                            \"encoding_format\": \"float\"\n",
    "                        }\n",
    "                    }\n",
    "                    batch.append(payload)\n",
    "                    total_jokes += 1\n",
    "\n",
    "                    # Write the batch to a file if it hits the batch size limit\n",
    "                    if len(batch) >= batch_size:\n",
    "                        output_file = f'{output_folder}/jokes_batch_{batch_number}.jsonl'\n",
    "                        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "                            for item in batch:\n",
    "                                json.dump(item, outfile)\n",
    "                                outfile.write('\\n')\n",
    "                        batch = []  # Reset batch\n",
    "                        batch_number += 1  # Increment batch number\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    logging.warning(f\"Invalid JSON in line: {line}\")\n",
    "                    continue\n",
    "\n",
    "        # Write any remaining jokes in the last batch\n",
    "        if batch:\n",
    "            output_file = f'{output_folder}/jokes_batch_{batch_number}.jsonl'\n",
    "            with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "                for item in batch:\n",
    "                    json.dump(item, outfile)\n",
    "                    outfile.write('\\n')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Input file not found: {input_file}\")\n",
    "        raise\n",
    "\n",
    "    return total_jokes\n",
    "\n",
    "\n",
    "def create_batch_jobs(batch_folder):\n",
    "    batch_input_files = []\n",
    "    for file in os.listdir(batch_folder):\n",
    "        if file.endswith('.jsonl'):\n",
    "            # Create a batch job using the new purpose, 'batch'\n",
    "            created_file = client.files.create(\n",
    "                file=open(f'{batch_folder}/{file}', \"rb\"),\n",
    "                purpose=\"batch\"  # Correct purpose here\n",
    "            )\n",
    "            batch_input_files.append(created_file)  # Append the `FileObject` directly\n",
    "\n",
    "    # Create embedding jobs from uploaded files\n",
    "    batch_file_ids = [batch_file.id for batch_file in batch_input_files]  # Access the `id` attribute\n",
    "    job_creations = []\n",
    "    for i, file_id in enumerate(batch_file_ids):\n",
    "        job_creations.append(client.batches.create(\n",
    "            input_file_id=file_id,\n",
    "            endpoint=\"/v1/embeddings\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\n",
    "                \"description\": f\"part_{i}_joke_embeddings\"\n",
    "            }\n",
    "        ))\n",
    "        logging.info(f\"Batch {i} has been submitted. Waiting for 26 minutes before submitting the next batch...\")\n",
    "        time.sleep(1600)  # 1200 seconds = 20 minutes\n",
    "    return job_creations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(input_file, output_file, batch_folder='./batch_files'):\n",
    "    # Ensure input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        logging.error(f\"Input file not found: {input_file}\")\n",
    "        return\n",
    "\n",
    "    # Prepare batch files\n",
    "    total_jokes = prepare_batch_files(input_file, output_folder=batch_folder)\n",
    "    logging.info(f\"Total jokes processed: {total_jokes}\")\n",
    "\n",
    "    # Create and monitor batch jobs\n",
    "    job_creations = create_batch_jobs(batch_folder)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "# Define your input and output file paths\n",
    "input_file = '../../data/processed/joke_selection_untaged.json'\n",
    "output_file = '../../data/processed/joke_embeddings.json'\n",
    "batch_folder = './batch_files'\n",
    "\n",
    "# Run the main function\n",
    "main(input_file, output_file, batch_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Join Embedding batches together and merge them with data set**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths to the necessary files and directories\n",
    "batch_folder = '../../data/processed/Embedded_batches'  # Adjust this to the actual path where the batch files are stored\n",
    "input_jokes_file = '../../data/processed/joke_selection_untaged.json'  # Path to the jokes file\n",
    "output_file = '../../data/processed/merged_joke_embeddings.json'  # Output file for the merged data\n",
    "missing_embeddings_file = '../../data/processed/missing_embeddings.json'  # File to store jokes missing embeddings\n",
    "\n",
    "# Function to load all batch files and extract embeddings\n",
    "def load_batch_files(batch_folder):\n",
    "    embedding_data = []\n",
    "    \n",
    "    # Loop through each file in the batch folder\n",
    "    for batch_file in os.listdir(batch_folder):\n",
    "        if batch_file.endswith('.jsonl'):\n",
    "            with open(os.path.join(batch_folder, batch_file), 'r', encoding='utf-8') as file:\n",
    "                # Read each line in the batch file\n",
    "                for line in file:\n",
    "                    data = json.loads(line)\n",
    "                    # Extract custom_id (joke id) and the embedding from the batch file\n",
    "                    custom_id = data['custom_id']\n",
    "                    embedding = data['response']['body']['data'][0]['embedding']\n",
    "                    embedding_data.append({\n",
    "                        'id': custom_id, \n",
    "                        'embedding': embedding\n",
    "                    })\n",
    "    \n",
    "    # Convert embedding data to a DataFrame for easy merging later\n",
    "    return pd.DataFrame(embedding_data)\n",
    "\n",
    "# Function to load jokes from the jokes file\n",
    "def load_jokes(input_jokes_file):\n",
    "    jokes_data = []\n",
    "    with open(input_jokes_file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            joke = json.loads(line)\n",
    "            jokes_data.append(joke)\n",
    "    \n",
    "    # Convert joke data to a DataFrame for easy merging\n",
    "    return pd.DataFrame(jokes_data)\n",
    "\n",
    "\n",
    "\n",
    "# Function to find jokes missing embeddings and save them to a file\n",
    "def find_missing_embeddings(merged_df, missing_embeddings_file):\n",
    "    # Find jokes where the embedding column is NaN\n",
    "    missing_embeddings_df = merged_df[merged_df['embedding'].isnull()]\n",
    "    \n",
    "    # Save the jokes missing embeddings to a file\n",
    "    if not missing_embeddings_df.empty:\n",
    "        missing_embeddings_df.to_json(missing_embeddings_file, orient='records', lines=True)\n",
    "        print(f\"Jokes missing embeddings saved to {missing_embeddings_file}\")\n",
    "    else:\n",
    "        print(\"No missing embeddings found.\")\n",
    "\n",
    "# Main function to perform the entire process\n",
    "def main(batch_folder, input_jokes_file, output_file, missing_embeddings_file=None):\n",
    "    print(\"Loading batch files and extracting embeddings...\")\n",
    "    embeddings_df = load_batch_files(batch_folder)\n",
    "    \n",
    "    print(\"Loading jokes from jokes file...\")\n",
    "    jokes_df = load_jokes(input_jokes_file)\n",
    "    \n",
    "    print(\"Merging jokes with embeddings...\")\n",
    "    merged_df=jokes_df.merge(embeddings_df, on='id', how='left')\n",
    "    \n",
    "    print(f\"Saving merged data to {output_file}...\")\n",
    "    # Save the merged data to a new JSON file, each joke on a new line\n",
    "    merged_df.to_json(output_file, orient='records', lines=True)\n",
    "    \n",
    "    # Optionally check for missing embeddings\n",
    "    if missing_embeddings_file:\n",
    "        print(\"Checking for missing embeddings...\")\n",
    "        find_missing_embeddings(merged_df, missing_embeddings_file)\n",
    "    \n",
    "    print(\"Process completed!\")\n",
    "\n",
    "# Run the main function\n",
    "main(batch_folder, input_jokes_file, output_file, missing_embeddings_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Categorizing the Jokes**\n",
    "\n",
    "In this step, we'll use OpenAI's GPT model to categorize our jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load .env file for the OpenAI API key\n",
    "load_dotenv()\n",
    "\n",
    "# Set up the OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def prepare_batch_files(input_file, batch_size=7000, output_folder='./batch_files'):\n",
    "    print(\"hello\")\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    batch_number = 0\n",
    "    batch = []\n",
    "    total_jokes = 0\n",
    "\n",
    "    prompt_template = \"\"\"Classify the following joke into one of these categories:\n",
    "Animal, Puns, Dad, Knock-Knock, One-Liners, Dark Humor, Political, Marriage, Work/Office, Tech, Ethnic, Kids, Doctor/Health, Lawyer, Food, Blonde, Yo Mama, School, Relationship, Religious, Sports, Punishments/Consequences, Celebrity, In-Law, Science, Insult\n",
    "\n",
    "Joke: {joke}\n",
    "\n",
    "Output: Return only the category name, without explanation.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "            for line in tqdm(infile, desc=\"Preparing batches\"):\n",
    "                try:\n",
    "                    joke = json.loads(line)\n",
    "                    joke_text = joke['full_joke']\n",
    "                    joke_id = joke['id']\n",
    "\n",
    "                    prompt = prompt_template.format(joke=joke_text)\n",
    "\n",
    "                    # Create the JSONL structure for classification\n",
    "                    payload = {\n",
    "                        \"custom_id\": joke_id,\n",
    "                        \"method\": \"POST\",\n",
    "                        \"url\": \"/v1/chat/completions\",\n",
    "                        \"body\": {\n",
    "                            \"model\": \"gpt-3.5-turbo-0125\",\n",
    "                            \"messages\": [\n",
    "                                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                                {\"role\": \"user\", \"content\": prompt}\n",
    "                            ],\n",
    "                            \"max_tokens\": 100\n",
    "                        }\n",
    "                    }\n",
    "                    batch.append(payload)\n",
    "                    total_jokes += 1\n",
    "\n",
    "                    # Write the batch to a file if it hits the batch size limit\n",
    "                    if len(batch) >= batch_size:\n",
    "                        output_file = f'{output_folder}/jokes_batch_{batch_number}.jsonl'\n",
    "                        logging.info(f'Writing batch file {output_file} with {len(batch)} jokes')  # Log batch size\n",
    "                        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "                            for item in batch:\n",
    "                                json.dump(item, outfile)\n",
    "                                outfile.write('\\n')\n",
    "                        batch = []  # Reset batch\n",
    "                        batch_number += 1  # Increment batch number\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    logging.warning(f\"Invalid JSON in line: {line}\")\n",
    "                    continue\n",
    "\n",
    "        # Write any remaining jokes in the last batch\n",
    "        if batch:\n",
    "            output_file = f'{output_folder}/jokes_batch_{batch_number}.jsonl'\n",
    "            logging.info(f'Writing final batch file {output_file} with {len(batch)} jokes')\n",
    "            with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "                for item in batch:\n",
    "                    json.dump(item, outfile)\n",
    "                    outfile.write('\\n')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Input file not found: {input_file}\")\n",
    "        raise\n",
    "\n",
    "    return total_jokes\n",
    "\n",
    "def create_batch_jobs(batch_folder):\n",
    "    batch_input_files = []\n",
    "    for file in os.listdir(batch_folder):\n",
    "        if file.endswith('.jsonl'):\n",
    "            # Create a batch job using OpenAI\n",
    "            created_file = client.files.create(\n",
    "                file=open(f'{batch_folder}/{file}', \"rb\"),\n",
    "                purpose=\"batch\"\n",
    "            )\n",
    "            batch_input_files.append(created_file)  # Append the `FileObject` directly\n",
    "\n",
    "    # Create chat completions batch jobs from uploaded files\n",
    "    batch_file_ids = [batch_file.id for batch_file in batch_input_files]\n",
    "    job_creations = []\n",
    "    for i, file_id in enumerate(batch_file_ids):\n",
    "        job_creations.append(client.batches.create(\n",
    "            input_file_id=file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\n",
    "                \"description\": f\"part_{i}_joke_classifications\"\n",
    "            }\n",
    "        ))\n",
    "        logging.info(f\"Batch {i} has been submitted. Waiting for 33 minutes before submitting the next batch...\")\n",
    "        time.sleep(400)  # 900 seconds = ~33 minutes\n",
    "    return job_creations\n",
    "\n",
    "def main(input_file, batch_folder='./batch_files'):\n",
    "    # Ensure input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        logging.error(f\"Input file not found: {input_file}\")\n",
    "        return\n",
    "\n",
    "    # Prepare batch files\n",
    "    total_jokes = prepare_batch_files(input_file, output_folder=batch_folder)\n",
    "    logging.info(f\"Total jokes processed: {total_jokes}\")\n",
    "\n",
    "    # Create and submit batch jobs\n",
    "    create_batch_jobs(batch_folder)\n",
    "\n",
    "# Define your input file path\n",
    "input_file = '../../data/processed/joke_selection_untaged.json'\n",
    "batch_folder = './batch_files'\n",
    "\n",
    "# Run the main function\n",
    "main(input_file, batch_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Merge Embeddings with Categories to produce the final dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths to the necessary files and directories\n",
    "categorized_batches_folder = '../../data/processed/Categorized_batches'  # Path to the categorized batches\n",
    "input_file = '../../data/processed/merged_joke_embeddings.json'  # Path to the jokes file with the embeddings\n",
    "final_output_file = '../../data/processed/final_joke_dataset.json'  # Output file for the merged data\n",
    "missing_categories_file = '../../data/processed/missing_categories.json'  # File to store jokes missing categories\n",
    "\n",
    "# Function to load categorized joke batches\n",
    "def load_categorized_batches(batch_folder):\n",
    "    categorized_data = []\n",
    "    \n",
    "    # Loop through each file in the batch folder\n",
    "    for batch_file in os.listdir(batch_folder):\n",
    "        if batch_file.endswith('.jsonl'):\n",
    "            with open(os.path.join(batch_folder, batch_file), 'r', encoding='utf-8') as file:\n",
    "                # Read each line in the batch file\n",
    "                for line in file:\n",
    "                    data = json.loads(line)\n",
    "                    # Extract custom_id (joke id) and the categorized content from the batch file\n",
    "                    custom_id = data['custom_id']\n",
    "                    category = data['response']['body']['choices'][0]['message']['content']\n",
    "                    categorized_data.append({\n",
    "                        'id': custom_id,\n",
    "                        'category': category.strip()  # Strip to remove extra spaces\n",
    "                    })\n",
    "    \n",
    "    # Convert categorized data to a DataFrame for easy merging later\n",
    "    return pd.DataFrame(categorized_data)\n",
    "\n",
    "# Function to load jokes with embeddings\n",
    "def load_jokes_with_embeddings(input_file):\n",
    "    jokes_data = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            joke = json.loads(line)\n",
    "            jokes_data.append(joke)\n",
    "    \n",
    "    # Convert joke data to a DataFrame for easy merging\n",
    "    return pd.DataFrame(jokes_data)\n",
    "\n",
    "# Function to find jokes missing categories and save them to a file\n",
    "def find_missing_categories(merged_df, missing_categories_file):\n",
    "    # Find jokes where the category column is NaN\n",
    "    missing_categories_df = merged_df[merged_df['category'].isnull()]\n",
    "    \n",
    "    # Save the jokes missing categories to a file\n",
    "    if not missing_categories_df.empty:\n",
    "        missing_categories_df.to_json(missing_categories_file, orient='records', lines=True)\n",
    "        print(f\"Jokes missing categories saved to {missing_categories_file}\")\n",
    "    else:\n",
    "        print(\"No jokes missing categories found.\")\n",
    "\n",
    "# Main function to perform the entire process\n",
    "def main(categorized_batches_folder, input_file, final_output_file, missing_categories_file=None):\n",
    "    print(\"Loading categorized batch files and extracting categories...\")\n",
    "    categorized_df = load_categorized_batches(categorized_batches_folder)\n",
    "    \n",
    "    print(\"Loading jokes with embeddings...\")\n",
    "    jokes_df = load_jokes_with_embeddings(input_file)\n",
    "    \n",
    "    print(\"Merging jokes with categories...\")\n",
    "    merged_df = jokes_df.merge(categorized_df, on='id', how='left')\n",
    "    \n",
    "    print(f\"Saving merged data to {final_output_file}...\")\n",
    "    # Save the merged data to a new JSON file, each joke on a new line\n",
    "    merged_df.to_json(final_output_file, orient='records', lines=True)\n",
    "    \n",
    "    # Optionally check for missing categories\n",
    "    if missing_categories_file:\n",
    "        print(\"Checking for jokes missing categories...\")\n",
    "        find_missing_categories(merged_df, missing_categories_file)\n",
    "    \n",
    "    print(\"Process completed!\")\n",
    "\n",
    "# Run the main function\n",
    "main(categorized_batches_folder, input_file, final_output_file, missing_categories_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
